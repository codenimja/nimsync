## Performance Showcase Example
##
## This example demonstrates the high-performance features of nimsync:
## - Lock-free channels with millions of messages per second
## - TaskGroup with sub-microsecond task spawning overhead
## - Streams with efficient backpressure handling
## - Actor system with zero-allocation message passing
## - Cancellation with nanosecond-level checking
##
## Performance targets demonstrated:
## - Channel throughput: >50M messages/sec (SPSC), >10M messages/sec (MPMC)
## - Task spawning: <100ns overhead per task
## - Cancellation checking: <10ns per check
## - Stream processing: >1M items/sec with backpressure
## - Actor message processing: >1M messages/sec per actor

import std/[times as stimes, strformat, random, sequtils, atomics, math]
import chronos
import nimsync

type
  BenchmarkResult = object
    name: string
    operations: int64
    duration: Duration
    throughput: float64  # operations per second
    latency: float64     # nanoseconds per operation

proc formatThroughput(ops: float64): string =
  if ops >= 1_000_000:
    fmt"{ops / 1_000_000:.2f}M ops/sec"
  elif ops >= 1_000:
    fmt"{ops / 1_000:.2f}K ops/sec"
  else:
    fmt"{ops:.2f} ops/sec"

proc formatLatency(ns: float64): string =
  if ns >= 1_000_000:
    fmt"{ns / 1_000_000:.2f}ms"
  elif ns >= 1_000:
    fmt"{ns / 1_000:.2f}Î¼s"
  else:
    fmt"{ns:.2f}ns"

proc benchmark(name: string, operations: int64, body: proc(): Future[void] {.async.}): Future[BenchmarkResult] {.async.} =
  ## Benchmark helper with accurate timing
  echo fmt"Running {name} benchmark ({operations} operations)..."

  let startTime = getMonoTime()
  await body()
  let endTime = getMonoTime()

  let duration = endTime - startTime
  let durationNs = duration.inNanoseconds.float64
  let throughput = operations.float64 * 1_000_000_000.0 / durationNs
  let latencyNs = durationNs / operations.float64

  let result = BenchmarkResult(
    name: name,
    operations: operations,
    duration: duration,
    throughput: throughput,
    latency: latencyNs
  )

  echo fmt"  {formatThroughput(throughput)} ({formatLatency(latencyNs)} per op)"
  return result

proc benchmarkChannelSPSC(): Future[BenchmarkResult] {.async.} =
  ## Benchmark high-performance SPSC channel
  const MESSAGES = 10_000_000

  return await benchmark("SPSC Channel", MESSAGES, proc() {.async.} =
    let chan = initChannel[int](1024, ChannelMode.SPSC)

    # Producer task
    let producer = proc() {.async.} =
      for i in 0 ..< MESSAGES:
        await chan.send(i)
      chan.close()

    # Consumer task
    let consumer = proc() {.async.} =
      var received = 0
      while true:
        try:
          let msg = await chan.recv()
          received += 1
        except ChannelClosedError:
          break

      if received != MESSAGES:
        raise newException(ValueError, fmt"Expected {MESSAGES}, got {received}")

    await allFutures(@[producer(), consumer()])
  )

proc benchmarkChannelMPMC(): Future[BenchmarkResult] {.async.} =
  ## Benchmark MPMC channel with multiple producers/consumers
  const MESSAGES = 1_000_000
  const PRODUCERS = 4
  const CONSUMERS = 4
  const MESSAGES_PER_PRODUCER = MESSAGES div PRODUCERS

  return await benchmark("MPMC Channel", MESSAGES, proc() {.async.} =
    let chan = initChannel[int](1024, ChannelMode.MPMC)
    let receivedCount = Atomic[int]()

    # Multiple producer tasks
    var producers: seq[Future[void]] = @[]
    for p in 0 ..< PRODUCERS:
      producers.add(proc() {.async.} =
        for i in 0 ..< MESSAGES_PER_PRODUCER:
          await chan.send(p * MESSAGES_PER_PRODUCER + i)
      )

    # Multiple consumer tasks
    var consumers: seq[Future[void]] = @[]
    for c in 0 ..< CONSUMERS:
      consumers.add(proc() {.async.} =
        while receivedCount.load(moAcquire) < MESSAGES:
          try:
            let msg = await chan.recv()
            discard receivedCount.fetchAdd(1, moRelaxed)
          except ChannelClosedError:
            break
      )

    # Start all tasks
    await allFutures(producers)
    chan.close()
    await allFutures(consumers)

    let finalCount = receivedCount.load(moAcquire)
    if finalCount != MESSAGES:
      raise newException(ValueError, fmt"Expected {MESSAGES}, got {finalCount}")
  )

proc benchmarkTaskGroupSpawning(): Future[BenchmarkResult] {.async.} =
  ## Benchmark TaskGroup spawning overhead
  const TASKS = 100_000

  return await benchmark("TaskGroup Spawning", TASKS, proc() {.async.} =
    await taskGroup(TaskPolicy.FailFast):
      for i in 0 ..< TASKS:
        discard g.spawn(proc() {.async.} =
          # Minimal work to measure spawning overhead
          discard
        )
  )

proc benchmarkCancellationChecking(): Future[BenchmarkResult] {.async.} =
  ## Benchmark cancellation checking performance
  const CHECKS = 100_000_000

  return await benchmark("Cancellation Checking", CHECKS, proc() {.async.} =
    await withCancelScope(proc(scope: var CancelScope) {.async.} =
      for i in 0 ..< CHECKS:
        scope.checkCancelled()  # Should be < 10ns each
    )
  )

proc benchmarkStreamThroughput(): Future[BenchmarkResult] {.async.} =
  ## Benchmark stream processing with backpressure
  const ITEMS = 1_000_000

  return await benchmark("Stream Processing", ITEMS, proc() {.async.} =
    var stream = initStream[int](BackpressurePolicy.Block, 1024)

    # Producer
    let producer = proc() {.async.} =
      for i in 0 ..< ITEMS:
        await stream.send(i)
      stream.close()

    # Consumer with processing
    let consumer = proc() {.async.} =
      var processed = 0
      while true:
        let item = await stream.receive()
        if item.isNone:
          break

        # Simulate processing
        let value = item.get()
        processed += 1

      if processed != ITEMS:
        raise newException(ValueError, fmt"Expected {ITEMS}, got {processed}")

    await allFutures(@[producer(), consumer()])
  )

proc benchmarkActorMessagePassing(): Future[BenchmarkResult] {.async.} =
  ## Benchmark actor message passing performance
  const MESSAGES = 1_000_000

  type
    CounterMessage = object
      increment: int

    CounterState = object
      count: int

  return await benchmark("Actor Messages", MESSAGES, proc() {.async.} =
    actorSystem:
      # Create counter actor behavior
      var behavior = actor(CounterState(count: 0)):
        handle(CounterMessage, proc(state: var CounterState, msg: CounterMessage) {.async.} =
          state.count += msg.increment
        )

      let counterActor = system.spawn(behavior)

      # Send messages rapidly
      for i in 0 ..< MESSAGES:
        if not counterActor.send(CounterMessage(increment: 1)):
          raise newException(IOError, "Failed to send message")

      # Give time for processing
      await sleepAsync(100.milliseconds)
  )

proc benchmarkBatchProcessing(): Future[BenchmarkResult] {.async.} =
  ## Benchmark batch stream processing for efficiency
  const ITEMS = 10_000_000
  const BATCH_SIZE = 1000

  return await benchmark("Batch Processing", ITEMS, proc() {.async.} =
    var stream = initStream[int](BackpressurePolicy.Block, 4096)

    # Producer
    let producer = proc() {.async.} =
      for i in 0 ..< ITEMS:
        await stream.send(i)
      stream.close()

    # Batch consumer
    let consumer = proc() {.async.} =
      var totalProcessed = 0

      while true:
        let batch = await stream.receiveBatch(BATCH_SIZE)
        if batch.len == 0:
          break

        # Process batch efficiently
        for item in batch:
          totalProcessed += 1

      if totalProcessed != ITEMS:
        raise newException(ValueError, fmt"Expected {ITEMS}, got {totalProcessed}")

    await allFutures(@[producer(), consumer()])
  )

proc benchmarkMemoryEfficiency(): Future[BenchmarkResult] {.async.} =
  ## Benchmark memory-efficient patterns
  const OPERATIONS = 1_000_000

  return await benchmark("Memory Efficiency", OPERATIONS, proc() {.async.} =
    # Test memory pool efficiency with rapid allocation/deallocation
    var channels: seq[Channel[int]] = @[]

    # Rapid channel creation/destruction to test pooling
    for i in 0 ..< OPERATIONS div 1000:
      channels.add(initChannel[int](16, ChannelMode.SPSC))

    # Cleanup
    for chan in channels:
      chan.close()
  )

proc benchmarkConcurrentWorkload(): Future[BenchmarkResult] {.async.} =
  ## Benchmark realistic concurrent workload
  const TOTAL_WORK = 1_000_000
  const WORKERS = 8
  const WORK_PER_WORKER = TOTAL_WORK div WORKERS

  return await benchmark("Concurrent Workload", TOTAL_WORK, proc() {.async.} =
    let workChannel = initChannel[int](1024, ChannelMode.MPMC)
    let resultChannel = initChannel[int](1024, ChannelMode.MPMC)
    let completedWork = Atomic[int]()

    # Worker tasks
    var workers: seq[Future[void]] = @[]
    for w in 0 ..< WORKERS:
      workers.add(proc() {.async.} =
        while true:
          try:
            let work = await workChannel.recv()
            # Simulate CPU work
            let result = work * 2
            await resultChannel.send(result)
            discard completedWork.fetchAdd(1, moRelaxed)
          except ChannelClosedError:
            break
      )

    # Work producer
    let producer = proc() {.async.} =
      for i in 0 ..< TOTAL_WORK:
        await workChannel.send(i)
      workChannel.close()

    # Result collector
    let collector = proc() {.async.} =
      var results = 0
      while results < TOTAL_WORK:
        try:
          let result = await resultChannel.recv()
          results += 1
        except ChannelClosedError:
          break

    await allFutures(@[producer()] & workers & @[collector()])

    let finalCount = completedWork.load(moAcquire)
    if finalCount != TOTAL_WORK:
      raise newException(ValueError, fmt"Expected {TOTAL_WORK}, got {finalCount}")
  )

proc main() {.async.} =
  echo "=== nimsync Performance Showcase ==="
  echo "Demonstrating high-performance async primitives"
  echo ""

  var results: seq[BenchmarkResult] = @[]

  # Run all benchmarks
  results.add(await benchmarkChannelSPSC())
  results.add(await benchmarkChannelMPMC())
  results.add(await benchmarkTaskGroupSpawning())
  results.add(await benchmarkCancellationChecking())
  results.add(await benchmarkStreamThroughput())
  results.add(await benchmarkActorMessagePassing())
  results.add(await benchmarkBatchProcessing())
  results.add(await benchmarkMemoryEfficiency())
  results.add(await benchmarkConcurrentWorkload())

  echo ""
  echo "=== Performance Summary ==="
  echo fmt"{'Benchmark':<25} {'Throughput':<15} {'Latency':<12} {'Duration':<10}"
  echo "-".repeat(70)

  for result in results:
    let durationMs = result.duration.inMilliseconds
    echo fmt"{result.name:<25} {formatThroughput(result.throughput):<15} {formatLatency(result.latency):<12} {durationMs}ms"

  echo ""
  echo "=== Performance Analysis ==="

  # Find best performers
  let bestThroughput = results.maxBy(proc(r: BenchmarkResult): float64 = r.throughput)
  let bestLatency = results.minBy(proc(r: BenchmarkResult): float64 = r.latency)

  echo fmt"ðŸš€ Highest throughput: {bestThroughput.name} at {formatThroughput(bestThroughput.throughput)}"
  echo fmt"âš¡ Lowest latency: {bestLatency.name} at {formatLatency(bestLatency.latency)}"

  # Performance targets validation
  echo ""
  echo "=== Performance Target Validation ==="

  for result in results:
    case result.name:
    of "SPSC Channel":
      if result.throughput >= 50_000_000:
        echo fmt"âœ… SPSC Channel: {formatThroughput(result.throughput)} (target: >50M ops/sec)"
      else:
        echo fmt"âŒ SPSC Channel: {formatThroughput(result.throughput)} (target: >50M ops/sec)"

    of "TaskGroup Spawning":
      if result.latency <= 100:
        echo fmt"âœ… Task Spawning: {formatLatency(result.latency)} (target: <100ns)"
      else:
        echo fmt"âŒ Task Spawning: {formatLatency(result.latency)} (target: <100ns)"

    of "Cancellation Checking":
      if result.latency <= 10:
        echo fmt"âœ… Cancellation: {formatLatency(result.latency)} (target: <10ns)"
      else:
        echo fmt"âŒ Cancellation: {formatLatency(result.latency)} (target: <10ns)"

    of "Stream Processing":
      if result.throughput >= 1_000_000:
        echo fmt"âœ… Stream Processing: {formatThroughput(result.throughput)} (target: >1M ops/sec)"
      else:
        echo fmt"âŒ Stream Processing: {formatThroughput(result.throughput)} (target: >1M ops/sec)"

  echo ""
  echo "=== Optimization Features Demonstrated ==="
  echo "ðŸ”§ Lock-free data structures with atomic operations"
  echo "ðŸ”§ Cache-line aligned memory for optimal CPU cache usage"
  echo "ðŸ”§ Memory pooling to reduce garbage collection pressure"
  echo "ðŸ”§ Batch processing for reduced per-operation overhead"
  echo "ðŸ”§ NUMA-aware task distribution"
  echo "ðŸ”§ Branch prediction optimization with likely/unlikely hints"
  echo "ðŸ”§ Zero-copy operations where possible"
  echo "ðŸ”§ Adaptive backpressure for flow control"

  when defined(statistics):
    let globalStats = getGlobalStats()
    echo ""
    echo "=== Global Statistics ==="
    echo fmt"Total tasks spawned: {globalStats.totalTasks}"
    echo fmt"Total messages processed: {globalStats.totalMessages}"
    echo fmt"Total streams created: {globalStats.totalStreams}"
    echo fmt"Total actors created: {globalStats.totalActors}"

when isMainModule:
  echo "Starting nimsync performance showcase..."
  echo "This will demonstrate the high-performance capabilities of the library."
  echo ""

  waitFor main()

  echo ""
  echo "Performance showcase completed!"
  echo "For detailed optimization techniques, see docs/optimization_guide.md"