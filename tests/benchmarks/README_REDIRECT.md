# Benchmarks Location

## Official Benchmarks

**All benchmark implementations have been moved to:**
ðŸ‘‰ **https://github.com/codenimja/nimsync-benchmarks**

The separate repository provides:
- âœ… Community-driven benchmark contributions
- âœ… Continuous CI validation
- âœ… Performance regression tracking
- âœ… Cross-platform benchmark results
- âœ… Detailed methodology documentation

## What's Here

This directory contains **internal stress tests** for validation during development:

```
stress_tests/
â”œâ”€â”€ backpressure_test.nim       - Buffer overflow handling
â”œâ”€â”€ comprehensive_stress_test.nim - Combined load scenarios
â”œâ”€â”€ database_pool_test.nim      - Connection pooling validation
â”œâ”€â”€ failure_mode_test.nim       - Error handling under stress
â”œâ”€â”€ http_load_test.nim          - HTTP client stress testing
â”œâ”€â”€ long_running.nim            - 24-hour endurance test
â”œâ”€â”€ memory_pressure_test.nim    - GC pressure validation
â”œâ”€â”€ mixed_workload_test.nim     - Real-world mixed operations
â”œâ”€â”€ mpmc_stress_test.nim        - Future MPMC validation (SPSC only in v1.0.0)
â”œâ”€â”€ real_world_scenarios.nim    - Application-like workloads
â”œâ”€â”€ spawn_stress_test.nim       - Task spawn rate validation
â”œâ”€â”€ streaming_pipeline.nim      - Stream backpressure testing
â””â”€â”€ websocket_load_test.nim     - WebSocket concurrent connections
```

## Running Internal Stress Tests

```bash
# Run all stress tests
cd tests/benchmarks/stress_tests
./run_suite

# Or individual tests
nim c -r long_running.nim
nim c -r memory_pressure_test.nim
```

## Contributing Benchmarks

To contribute performance benchmarks or comparative studies, please submit PRs to:
**https://github.com/codenimja/nimsync-benchmarks**

See the [CONTRIBUTING.md](https://github.com/codenimja/nimsync-benchmarks/blob/main/CONTRIBUTING.md) guide for benchmark submission requirements.
