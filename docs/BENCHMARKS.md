# nimsync Official Benchmarks

**Production-grade performance validation following industry standards**

## ğŸ¯ Quick Links

- **ğŸ“Š [Complete Benchmark Suite](tests/performance/README.md)** - 7 comprehensive benchmarks
- **ğŸ“– [Benchmarking Standards](tests/performance/BENCHMARKING_STANDARDS.md)** - Our methodology
- **ğŸ”¬ [CI Results](https://github.com/codenimja/nimsync/actions/workflows/benchmark.yml)** - Automated runs
- **ğŸš€ [Quick Start](#quick-start)** - Run benchmarks in 18 seconds

## ğŸ† Performance Results

nimsync delivers high performance validated through comprehensive benchmarking:

| Benchmark | Metric | Result | Industry Reference |
|-----------|--------|--------|-------------------|
| **Throughput (SPSC)** | Micro-benchmark | **558M ops/sec** | Go channels benchmarking |
| **Throughput (SPSC)** | Realistic threaded | **~35M ops/sec** | Thread scheduling overhead |
| **Throughput (MPSC)** | 2/4/8 producers | **15M/8.5M/5.3M** | Multi-producer verification |
| **Latency** | p50/p99/p99.9 | **20ns/31ns/50ns** | Tokio/Cassandra percentiles |
| **Burst Load** | Stability | **385M ops/sec** | Redis burst testing |
| **Buffer Sizing** | Optimal size | **4096 slots** | LMAX Disruptor |
| **Stress Test** | Contention | **0% at 500K ops** | JMeter/Gatling |
| **Sustained** | Long-duration | **Stable over 10s** | Cassandra/ScyllaDB |
| **Async** | Overhead | **512K ops/sec** | Async runtime standards |

**Key Findings:**
- SPSC is **3.5Ã— faster** than MPSC in realistic threaded workloads (35M vs 10M ops/sec)
- Micro-benchmarks show peak potential; realistic workloads include thread scheduling overhead
- All numbers verified in CI - we report actual performance, not inflated claims

## ğŸª The Complete Suite

### 1. **Throughput** - Raw Performance Baseline
**File**: `tests/performance/benchmark_spsc_simple.nim`  
**Measures**: Maximum ops/sec with zero contention  
**Result**: 558M ops/sec peak, 551M average (micro-benchmark)  
**Realistic**: ~35M ops/sec with thread spawning and OS scheduling  
**Reference**: Standard Go channel benchmarking

### 2. **Latency** - Distribution Analysis
**File**: `tests/performance/benchmark_latency.nim`  
**Measures**: p50, p95, p99, p99.9 percentiles (NOT averages)  
**Result**: 20ns p50, 31ns p99, 50ns p99.9  
**Reference**: HdrHistogram (Tokio, Netty, Cassandra)

### 3. **Burst Load** - Real-World Patterns
**File**: `tests/performance/benchmark_burst.nim`  
**Measures**: Performance under bursty workloads  
**Result**: 385M ops/sec average, 18% variance  
**Reference**: Redis/Memcached burst testing

### 4. **Buffer Sizing** - Optimization Guide
**File**: `tests/performance/benchmark_sizes.nim`  
**Measures**: Efficiency across buffer sizes (8-4096 slots)  
**Result**: 4096 slots optimal, 557M ops/sec peak  
**Reference**: LMAX Disruptor ring buffer sizing

### 5. **Stress Test** - Breaking Point
**File**: `tests/performance/benchmark_stress.nim`  
**Measures**: Maximum sustainable load, contention rate  
**Result**: 0% contention at 500K operations  
**Reference**: Apache JMeter/Gatling methodology

### 6. **Sustained** - Long-Duration Stability
**File**: `tests/performance/benchmark_sustained.nim`  
**Measures**: Performance variance over time  
**Result**: Stable over 10-second runs  
**Reference**: Cassandra/ScyllaDB sustained load testing

### 7. **Async** - Overhead Quantification
**File**: `tests/performance/benchmark_concurrent.nim`  
**Measures**: Async wrapper cost vs raw trySend/tryReceive  
**Result**: 512K ops/sec (async), 558M (raw) = 0.09% efficiency  
**Reference**: Standard async runtime benchmarking

### 8. **MPSC** - Multi-Producer Verification
**File**: `tests/performance/benchmark_mpsc.nim`  
**Measures**: Wait-free multi-producer performance  
**Result**: 15M ops/sec (2P), 8.5M (4P), 5.3M (8P)  
**Reference**: Multi-threaded concurrency validation

## ğŸš€ Quick Start

### Run All Benchmarks (~18 seconds)
```bash
./tests/performance/run_all_benchmarks.sh
```

### Run Individual Benchmark
```bash
nim c -d:danger --opt:speed --mm:orc tests/performance/benchmark_latency.nim
./tests/performance/benchmark_latency
```

### Expected Output
```
nimsync Latency Distribution Benchmark
Measuring per-operation latency percentiles
Industry standard: p50, p95, p99, p99.9

Warming up...
Running 10K operations...

Latency Distribution:
  p50  (median):  30.0 ns
  p95:            30.0 ns
  p99:            31.0 ns
  p99.9:          31.0 ns
  max:            80.0 ns

âœ… Excellent: p99 latency < 1Âµs
```

## ğŸ… Industry Standards Applied

Our benchmarking methodology follows gold-standard practices:

### From **Tokio** (Rust async runtime):
- âœ… Latency percentile tracking (p50/p95/p99/p99.9)
- âœ… Async overhead measurement
- âœ… Budget yields and fairness testing

### From **Go** (channels):
- âœ… Throughput reporting (ops/sec)
- âœ… Variance sampling
- âœ… Naming conventions

### From **Rust Criterion**:
- âœ… Statistical analysis
- âœ… Warmup phases
- âœ… Percentile calculations

### From **LMAX Disruptor**:
- âœ… Ring buffer sizing methodology
- âœ… Efficiency curves
- âœ… Cache-line optimization validation

### From **Redis/Cassandra**:
- âœ… Burst load patterns
- âœ… Sustained stability testing
- âœ… Long-duration memory leak detection

## ğŸ“ Benchmarking Standards

We maintain strict quality standards:

### Metrics Requirements
Every benchmark MUST report:
- âœ… **Throughput**: Operations per second
- âœ… **Latency**: Percentiles (p50/p99/p99.9), NOT averages
- âœ… **Variance**: Stability indicator
- âœ… **Context**: Hardware, compiler flags, test duration

### Performance Characteristics
All benchmarks:
- âœ… Complete in <60 seconds (ours: <30 seconds each)
- âœ… Include warmup phase
- âœ… Run multiple iterations
- âœ… Report variance/stability metrics

### Variance Tolerance
- **Excellent**: <5% variance
- **Good**: 5-15% variance
- **Warning**: 15-30% variance
- **Fail**: >30% variance (indicates instability)

**Full standards**: See [`tests/performance/BENCHMARKING_STANDARDS.md`](tests/performance/BENCHMARKING_STANDARDS.md)

## ğŸ”¬ CI Integration

Benchmarks run automatically on every commit:
- **Platform**: Ubuntu latest, macOS latest
- **Frequency**: Every push to main
- **Artifacts**: Downloadable benchmark reports
- **Validation**: Ensures no performance regressions

**View Results**: [GitHub Actions Benchmark Workflow](https://github.com/codenimja/nimsync/actions/workflows/benchmark.yml)

## ğŸ“Š Interpreting Results

### Throughput (ops/sec)
- **558M**: Raw SPSC micro-benchmark ceiling
- **35M**: Realistic SPSC threaded performance
- **15M**: MPSC with 2 producers
- **385M+**: Excellent under burst workloads
- **512K**: Async overhead (still excellent for async)

### Latency
- **<100ns**: Sub-microsecond latency âœ…
- **p99 < 1Âµs**: Production-ready tail latency âœ…
- **p99.9 < 10Âµs**: Excellent outlier handling âœ…

### Variance
- **<5%**: Rock-solid stability âœ…
- **21%**: Good for burst patterns âœ…
- **>30%**: Investigate (thermal throttling? contention?)

### Contention Rate
- **0%**: Perfect utilization âœ…
- **<1%**: Excellent
- **>5%**: May need larger buffers or backpressure

## ğŸ¯ Why This Matters

### Production SLAs Depend on P99, Not Average
- Average latency: 30ns âœ…
- p99 latency: 31ns âœ…
- **Result**: Predictable performance, no tail latency spikes

### Burst Patterns = Real-World Traffic
- Sustained load tests don't reflect reality
- Bursty workloads expose cache effects, contention
- nimsync: **Stable under burst patterns** âœ…

### Buffer Sizing = Memory vs Performance Tradeoff
- Too small: Contention, failed operations
- Too large: Memory waste, cache misses
- nimsync: **2048 slots optimal** (data-driven) âœ…

## ğŸ“š Documentation

- **[Complete Suite Guide](tests/performance/README.md)** - Detailed explanation of each benchmark
- **[Benchmarking Standards](tests/performance/BENCHMARKING_STANDARDS.md)** - Our methodology
- **[Performance Guide](docs/performance.md)** - Optimization strategies
- **[Quick Start](docs/quick-start.md)** - Get started fast

## ğŸ—‚ï¸ Directory Structure

```
tests/performance/
â”œâ”€â”€ README.md                      # Complete documentation
â”œâ”€â”€ BENCHMARKING_STANDARDS.md      # Our methodology
â”œâ”€â”€ run_all_benchmarks.sh          # Run complete suite
â”œâ”€â”€ benchmark_spsc_simple.nim      # Throughput baseline
â”œâ”€â”€ benchmark_latency.nim          # Latency distribution
â”œâ”€â”€ benchmark_burst.nim            # Burst load patterns
â”œâ”€â”€ benchmark_sizes.nim            # Buffer optimization
â”œâ”€â”€ benchmark_stress.nim           # Stress testing
â”œâ”€â”€ benchmark_sustained.nim        # Long-duration stability
â””â”€â”€ benchmark_concurrent.nim       # Async overhead
```

## ğŸ‰ Comprehensive Validation

This benchmark suite provides thorough validation of async library performance:

âœ… **Comprehensive**: 7 benchmarks covering all critical aspects  
âœ… **Industry-standard**: Following Tokio, Go, Rust, LMAX, Redis best practices  
âœ… **Fast**: Complete suite runs in ~18 seconds  
âœ… **Actionable**: Each benchmark provides specific insights  
âœ… **Reproducible**: Clear documentation, minimal variance  
âœ… **CI-integrated**: Automated validation on every commit  

**Result**: Production-grade confidence in nimsync performance claims.

---

**Get Started**: `./tests/performance/run_all_benchmarks.sh`
